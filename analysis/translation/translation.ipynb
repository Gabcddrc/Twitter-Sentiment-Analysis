{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Non-English and Mixed-Language Hypothesis\n",
                "\n",
                "There appear to be a number of non-English and mixed-language tweets within our\n",
                "dataset. In particular, these can be found amongst tweets misclassified by\n",
                "BERTweet. Since BERTweet's embedding is trained on purely English tweets, we\n",
                "cannot expect to correctly understand and classify such non-English/mixed-language\n",
                "tweets without additional measures. We hypothesise that translating such tweets\n",
                "using a service such as Google Translate would increase the accuracy of BERTweet.\n",
                "\n",
                "For this we use the Google Translate API: https://codelabs.developers.google.com/codelabs/cloud-translation-python3"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# All results have been precomputed already and included within this repository.\n",
                "# This is done as some operations take extensive amounts of time and require\n",
                "# the user to setup Google Cloud Services. Nevertheless, the code within this\n",
                "# notebook can still be used to reproduce our results. The following flag, when\n",
                "# set to True will only use the precomputed data we provide.\n",
                "USE_PRECOMPUTED = True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Google project ID.\n",
                "%env PROJECT_ID=cli-project\n",
                "# Path to key.json containing credentials.\n",
                "%env GOOGLE_APPLICATION_CREDENTIALS=./key.json\n",
                "\n",
                "import sys\n",
                "import os\n",
                "from os import environ\n",
                "\n",
                "from google.cloud import translate\n",
                "\n",
                "import pandas as pd\n",
                "from IPython.display import display\n",
                "\n",
                "sys.path.append(\"../../src\")\n",
                "from data_processing.loading import load_train_valid_data, load_test_data\n",
                "\n",
                "project_id = environ.get(\"PROJECT_ID\", \"\")\n",
                "assert project_id\n",
                "parent = f\"projects/{project_id}\"\n",
                "client = translate.TranslationServiceClient()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Detect Languages\n",
                "\n",
                "The first step in testing our hypothesis is to identify non-English or\n",
                "mixed-language tweets. We do this to keep the translation workload to a minimum,\n",
                "as we will only translate non-English and mixed-language tweets. We further use\n",
                "this step to identify the language composition of our dataset."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Load random subset of BERTweet misclassified tweets.\n",
                "valid_misclass = pd.read_csv(\"bertweet_valid_misclass.csv\")\n",
                "\n",
                "# Load test tweets.\n",
                "path_to_dataset = os.path.join(os.pardir, os.pardir, \"dataset\")\n",
                "test = load_test_data(path_to_dataset)\n",
                "\n",
                "def detect_language(client, parent, text):\n",
                "    try:\n",
                "        response = client.detect_language(parent=parent, content=text)\n",
                "        num_langs_detected = len(response.languages)\n",
                "        first_detected_lang = response.languages[0].language_code\n",
                "        confidence_first_lang = response.languages[0].confidence\n",
                "    except:\n",
                "        return \",,\"\n",
                "    return f\"{num_langs_detected},{first_detected_lang},{confidence_first_lang}\"\n",
                "\n",
                "if not USE_PRECOMPUTED:\n",
                "    # Detect languages for misclassified validation tweets and test tweets.\n",
                "    # Save the results.\n",
                "    valid_misclass[\"detect_lang\"] = valid_misclass[\"tweet\"].apply(\n",
                "        lambda tweet: detect_language(client, parent, tweet)\n",
                "    )\n",
                "    valid_misclass.to_csv(\n",
                "        os.path.join(\"language_detected\", \"valid_misclass.csv\")\n",
                "    )\n",
                "\n",
                "    test[\"detect_lang\"] = test[\"tweet\"].apply(\n",
                "        lambda tweet: detect_language(client, parent, tweet)\n",
                "    )\n",
                "    test.to_csv(os.path.join(\"language_detected\", \"test.csv\"))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Translate Tweets\n",
                "\n",
                "Now that languages have been detected, filter out all tweets found to be\n",
                "non-English, i.e. completely non-English or mixed-language, and translate\n",
                "those."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Prepare the language detected tweets for translation and filter for the\n",
                "# non-English ones.\n",
                "\n",
                "# Misclassified validation tweets.\n",
                "valid_misclass = pd.read_csv(\n",
                "    os.path.join('language_detected', 'valid_misclass.csv'),\n",
                "    index_col='id'\n",
                ")\n",
                "\n",
                "valid_misclass['num_detect_langs'] = valid_misclass['detect_lang'].apply(\n",
                "    lambda stat: stat.split(',')[0]\n",
                ")\n",
                "valid_misclass['detected_lang'] = valid_misclass['detect_lang'].apply(\n",
                "    lambda stat: stat.split(',')[1]\n",
                ")\n",
                "valid_misclass['confidence'] = valid_misclass['detect_lang'].apply(\n",
                "    lambda stat: stat.split(',')[2]\n",
                ")\n",
                "\n",
                "del valid_misclass['detect_lang']\n",
                "del valid_misclass['Unnamed: 0']\n",
                "\n",
                "valid_foreign_lang = valid_misclass[valid_misclass['detected_lang'] != 'en'].copy()\n",
                "\n",
                "# Test tweets.\n",
                "test = pd.read_csv(\n",
                "    os.path.join('language_detected', 'test_lang_detected.csv'),\n",
                "    index_col='id'\n",
                ")\n",
                "\n",
                "test['num_detect_langs'] = test['detect_lang'].apply(\n",
                "    lambda stat: stat.split(',')[0]\n",
                ")\n",
                "test['detected_lang'] = test['detect_lang'].apply(\n",
                "    lambda stat: stat.split(',')[1]\n",
                ")\n",
                "test['confidence'] = test['detect_lang'].apply(\n",
                "    lambda stat: stat.split(',')[2]\n",
                ")\n",
                "\n",
                "del test['detect_lang']\n",
                "\n",
                "test_foreign_lang = test[test['detected_lang'] != 'en'].copy()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Translate the relevant tweets and store them for reclassification.\n",
                "\n",
                "def translate(text: str, target_language_code: str) -> str:\n",
                "    response = client.translate_text(\n",
                "        contents=[text],\n",
                "        target_language_code=target_language_code,\n",
                "        parent=parent,\n",
                "    )\n",
                "    return response.translations\n",
                "\n",
                "def translate_to_en(tweet: str) -> str:\n",
                "    raw_translation = translate(tweet, 'en')[0].translated_text\n",
                "    translation = raw_translation.replace('&#39;', \"'\")\n",
                "    translation = translation.replace('&lt;', '<')\n",
                "    translation = translation.replace('&gt;', '>')\n",
                "    translation = translation.replace('&quot;', '\"')\n",
                "    return translation\n",
                "\n",
                "if not USE_PRECOMPUTED:\n",
                "    # Misclassified validation tweets.\n",
                "    valid_foreign_lang['translation'] = valid_foreign_lang['tweet'].apply(\n",
                "        lambda tweet: translate_to_en(tweet)\n",
                "    )\n",
                "\n",
                "    translated_valid_misclass = valid_foreign_lang[['tweet', 'translation', 'label']].copy()\n",
                "    translated_valid_misclass['tweet'] = translated_valid_misclass['translation']\n",
                "\n",
                "    del translated_valid_misclass['translation']\n",
                "\n",
                "    translated_valid_misclass.to_csv(\n",
                "        os.path.join('translated', 'valid_misclass.csv')\n",
                "    )\n",
                "\n",
                "    # Test tweets.\n",
                "    test_foreign_lang['translation'] = test_foreign_lang['tweet'].apply(\n",
                "        lambda tweet: translate_to_en(tweet)\n",
                "    )\n",
                "\n",
                "    translated_test = test_foreign_lang[['tweet', 'translation']].copy()\n",
                "    translated_test['tweet'] = translated_test['translation']\n",
                "\n",
                "    del translated_test['translation']\n",
                "\n",
                "    translated_test.to_csv(os.path.join('translated', 'test.csv'))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Merging Re-classified Tweets\n",
                "\n",
                "Using the CSV files generated in the last section, we can reclassify the\n",
                "translated tweets to check if accuracy improved. What we do now is merge the\n",
                "classifications of the tranlated tweets with the classifications of the English\n",
                "tweets we have not changed anything for."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Merging re-classified validation tweets.\n",
                "bertweet_valid_pred = pd.read_csv(\n",
                "    os.path.join('predictions', 'bertweet_valid.csv'),\n",
                "    index_col='Id'\n",
                ")\n",
                "bertweet_valid_translated_pred = pd.read_csv(\n",
                "    os.path.join('predictions', 'bertweet_valid_translated.csv'),\n",
                "    index_col='Id'\n",
                ")\n",
                "\n",
                "path_to_dataset = os.path.join(os.pardir, os.pardir, 'dataset')\n",
                "train, valid = load_train_valid_data(path_to_dataset)\n",
                "\n",
                "bertweet_combined_valid_pred = bertweet_valid_pred.join(\n",
                "    bertweet_valid_translated_pred,\n",
                "    lsuffix='_original',\n",
                "    rsuffix='_translated'\n",
                ")\n",
                "\n",
                "bertweet_merged_valid_pred = bertweet_combined_valid_pred\n",
                "bertweet_merged_valid_pred['Prediction'] = bertweet_merged_valid_pred['Prediction_translated'].fillna(\n",
                "    bertweet_merged_valid_pred['Prediction_original']\n",
                ")\n",
                "bertweet_merged_valid_pred['Prediction'] = bertweet_merged_valid_pred['Prediction'].astype(int)\n",
                "\n",
                "bertweet_merged_valid_pred = bertweet_merged_valid_pred.join(valid)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Print summary statistics on accuracy before and after translation.\n",
                "valid_size = len(valid)\n",
                "\n",
                "num_bertweet_correct = len(\n",
                "    bertweet_merged_valid_pred[bertweet_merged_valid_pred['Prediction_original'] == bertweet_merged_valid_pred['label']]\n",
                ")\n",
                "\n",
                "num_bertweet_translated_correct = len(\n",
                "    bertweet_merged_valid_pred[bertweet_merged_valid_pred['Prediction'] == bertweet_merged_valid_pred['label']]\n",
                ")\n",
                "\n",
                "num_changed_pred = len(\n",
                "    bertweet_merged_valid_pred[bertweet_merged_valid_pred['Prediction_original'] != bertweet_merged_valid_pred['Prediction']]\n",
                ")\n",
                "\n",
                "print(f'BERTweet Original Accuracy:\\t\\t\\t{100 * num_bertweet_correct / valid_size}')\n",
                "print(f'BERTweet Translated Accuracy:\\t\\t\\t{100 * num_bertweet_translated_correct / valid_size}')\n",
                "print(f'Accuracy Difference Original vs. Translated:\\t0{100 * (num_bertweet_translated_correct - num_bertweet_correct) / valid_size}')\n",
                "print(f'Percentage of changed prediction:\\t\\t0{100 * num_changed_pred / valid_size}')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Merging re-classified test tweets.\n",
                "bertweet_test_pred = pd.read_csv(\n",
                "    os.path.join('predictions', 'bertweet_test.csv'),\n",
                "    index_col='Id'\n",
                ")\n",
                "bertweet_test_translated_pred = pd.read_csv(\n",
                "    os.path.join('predictions', 'bertweet_test_translated.csv'),\n",
                "    index_col='Id'\n",
                ")\n",
                "\n",
                "bertweet_joint_pred = bertweet_test_pred.join(\n",
                "    bertweet_test_translated_pred,\n",
                "    lsuffix='_original',\n",
                "    rsuffix='_translated'\n",
                ")\n",
                "bertweet_merged_pred = bertweet_joint_pred\n",
                "bertweet_merged_pred['Prediction'] = bertweet_merged_pred['Prediction_translated'].fillna(\n",
                "    bertweet_merged_pred['Prediction_original']\n",
                ")\n",
                "bertweet_merged_pred['Prediction'] = bertweet_merged_pred['Prediction'].astype(int)\n",
                "\n",
                "bertweet_new_pred = bertweet_merged_pred.copy()\n",
                "del bertweet_new_pred['Prediction_original']\n",
                "del bertweet_new_pred['Prediction_translated']\n",
                "if not USE_PRECOMPUTED:\n",
                "    bertweet_new_pred.to_csv(\n",
                "        os.path.join('predictions', 'bertweet_test_merged.csv')\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}